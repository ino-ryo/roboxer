{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/punch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/punch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/punch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/punch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/punch/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import cv2\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ast\n",
    "import pyrealsense2 as rs\n",
    "import serial\n",
    "import time\n",
    "\n",
    "FRAME_WIDTH = 640\n",
    "FRAME_HEIGHT = 480\n",
    "GST_STR = 'nvarguscamerasrc \\\n",
    "    ! video/x-raw(memory:NVMM), width=3280, height=2464, format=(string)NV12, framerate=(fraction)30/1 \\\n",
    "    ! nvvidconv ! video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx \\\n",
    "    ! videoconvert \\\n",
    "    ! appsink' % (FRAME_WIDTH, FRAME_HEIGHT)\n",
    "WINDOW_NAME = 'TF-TRT Object Detection'\n",
    "MODEL_FILE = './ssd_inception_v2_coco_trt.pb'\n",
    "LABEL_FILE = './coco-labels-paper.txt'\n",
    "\n",
    "def load_graph_def(file):\n",
    "    with tf.gfile.GFile(file, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    return graph_def\n",
    "\n",
    "def load_labels(file):\n",
    "    labels = ['unlabeled']\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f.read().splitlines():\n",
    "            labels.append(line)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph definition...Done.\n",
      "Importing graph definition to TensorFlow...Done.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A simple python application to detect objects from camera captured image\n",
    "using TF-TRT for NVIDIA Jetson Nano Developer Kit.\n",
    "This application assumes the TensorRT optimized ssd_mobilenet_v1_coco model.\n",
    "Refer to the NVIDIA-AI-IOT/tf_trt_models GitHub ripository for details on \n",
    "the model.\n",
    "'''\n",
    "\n",
    "labels = load_labels(LABEL_FILE)\n",
    "num_labels = len(labels)\n",
    "\n",
    "print('Loading graph definition...', end = '', flush = True)\n",
    "trt_graph_def = load_graph_def(MODEL_FILE)\n",
    "print('Done.')\n",
    "\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_sess = tf.Session(config = tf_config)\n",
    "print('Importing graph definition to TensorFlow...', \\\n",
    "    end = '', flush = True)\n",
    "tf.import_graph_def(trt_graph_def, name = '')\n",
    "print('Done.')\n",
    "\n",
    "input_names = ['image_tensor']\n",
    "tf_input = tf_sess.graph.get_tensor_by_name(input_names[0] + ':0')\n",
    "tf_scores = tf_sess.graph.get_tensor_by_name('detection_scores:0')\n",
    "tf_boxes = tf_sess.graph.get_tensor_by_name('detection_boxes:0')\n",
    "tf_classes = tf_sess.graph.get_tensor_by_name('detection_classes:0')\n",
    "tf_num_detections = tf_sess.graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "#print('Configuring camera...', end = '', flush = True)\n",
    "#cap = cv2.VideoCapture(GST_STR, cv2.CAP_GSTREAMER)\n",
    "#print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e7154db02d9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         = tf_sess.run( \\\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtf_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_num_detections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             feed_dict={tf_input: imgRs[None, ...]})\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# index by 0 to remove batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "profile = pipeline.start(config)\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "alp = 0.2\n",
    "res = 1 - alp**0.5\n",
    "val_size = 2\n",
    "values = [0,0]\n",
    "threshold = 3.\n",
    "devide = 50\n",
    "#with serial.Serial('/dev/ttyACM0', 9600,timeout = 1)  as sr:\n",
    "with serial.Serial('/dev/ttyACM0', 115200,timeout=1)  as ser:\n",
    "\n",
    "#with serial.Serial('/dev/ttyUSB0', 9600,timeout = 1)  as ser:\n",
    "    while True:\n",
    "        t1 = time.time()\n",
    "        # Caputure frame\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        RGB_frame = frames.get_color_frame()\n",
    "        img = np.asanyarray(RGB_frame.get_data())\n",
    "        imgConv = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "        imgRs = cv2.resize(imgConv, (300, 300))\n",
    "        #imgRs = cv2.resize(imgConv, (640, 480))\n",
    "        if not depth_frame or not RGB_frame:\n",
    "            continue\n",
    "        # Do inference\n",
    "        scores, boxes, classes, num_detections \\\n",
    "        = tf_sess.run( \\\n",
    "            [tf_scores, tf_boxes, tf_classes, tf_num_detections], \\\n",
    "            feed_dict={tf_input: imgRs[None, ...]})\n",
    "\n",
    "        boxes = boxes[0] # index by 0 to remove batch dimension\n",
    "        scores = scores[0]\n",
    "        classes = classes[0]\n",
    "        num_detections = num_detections[0]\n",
    "        ls_d = []\n",
    "        ls_inf = []\n",
    "        ls_box = []\n",
    "        ls_ct = []\n",
    "        #print(1)\n",
    "        for i in range(int(num_detections)):\n",
    "            # Look up label string\n",
    "            \n",
    "            class_id = int(classes[i])\n",
    "            label = labels[class_id] if class_id < num_labels else 'unlabeled'\n",
    "            if label == \"person\":\n",
    "                depth_image = np.asanyarray(depth_frame.get_data())\n",
    "                # Get score\n",
    "                score = scores[i]\n",
    "                # Draw bounding box\n",
    "                box = boxes[i] * np.array( \\\n",
    "                    [FRAME_HEIGHT, FRAME_WIDTH, FRAME_HEIGHT, FRAME_WIDTH])\n",
    "                box = box.astype(np.int)\n",
    "                x1=box[1]\n",
    "                y1=box[0]\n",
    "                x2=box[3]\n",
    "                y2=box[2]\n",
    "                lx = (x2 - x1)\n",
    "                ud = int(lx*res/2)\n",
    "                u1 = x1 + ud\n",
    "                u2 = x2 - ud\n",
    "                ly = y2 - y1\n",
    "                vd = int(ly*res/2)\n",
    "                v1 = y1 + vd\n",
    "                v2 = y2 - vd\n",
    "                #cv2.rectangle(img,(box[1], box[0]), (box[3], box[2]), (0, 255, 0), 3)\n",
    "                #cv2.rectangle(img,(box[1], box[0]+10), (box[3], box[2]-30), (255, 0, 0), 3)\n",
    "                #cv2.rectangle(img, (u1, v1), (u2, v2), (255, 0, 0), 3)\n",
    "                cx = int(lx/2)\n",
    "                cy = int(ly/2)\n",
    "                #depth = depth_image[cx,cy].astype(float)\n",
    "                d_box = depth_image[v1:v2,u1:u2]\n",
    "                #send center val\n",
    "                center =(x1 + x2) / 2\n",
    "                #distance = depth * depth_scale\n",
    "                distance = np.mean(d_box) * depth_scale\n",
    "                # Put label near bounding box\n",
    "                inf = '%s: %f' % (label, score)\n",
    "                d = distance\n",
    "                ls_d.append(d)\n",
    "                ls_inf.append(inf)\n",
    "                ls_box.append(box[0:4])\n",
    "                ls_ct.append(center)\n",
    "                #print(inf)\n",
    "        #print(2)\n",
    "        if ls_d != []:   \n",
    "            min_i = ls_d.index(min(ls_d))\n",
    "            mbox = ls_box[min_i]\n",
    "                        #min_center =ls_ct[min_i]\n",
    "            min_d=ls_d[min_i]\n",
    "            if min_d<=threshold:\n",
    "                min_d = int(255*min_d/4.)\n",
    "                center = ls_ct[min_i]\n",
    "                center =int( devide * (center /640.) )\n",
    "                values = [min_d,center]\n",
    "                for i in range(val_size):\n",
    "                    #print(3)\n",
    "                    head = 128+i\n",
    "                    high = (values[i] >> 7) & 127\n",
    "                    low  = values[i] & 127\n",
    "                    #print(4)\n",
    "                    headByte = head.to_bytes(1, 'big')\n",
    "                    highByte = high.to_bytes(1, 'big')\n",
    "                    lowByte = low.to_bytes(1, 'big')\n",
    "                    ser.write(headByte)\n",
    "                    ser.write(highByte)\n",
    "                    ser.write(lowByte)\n",
    "                    #print(5)\n",
    "                #print(values)\n",
    "                cv2.putText(img, ls_inf[min_i], (mbox[1], mbox[2]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "                cv2.putText(img, 'posi: %f' %(center-devide/2), (mbox[1], mbox[2]-25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Show image\n",
    "        cv2.imshow(WINDOW_NAME, img)\n",
    "        \n",
    "        # Check if user hits ESC key to exit\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27: # ESC \n",
    "            break\n",
    "        t2 = time.time()\n",
    "        \n",
    "        \n",
    "        #print('iteration')\n",
    "ser.close() \n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser.close() \n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38795068116599024, 48]\n",
      "[0.3729007980908779, 45]\n",
      "[0.40504452473308716, 46]\n",
      "[1.5096617258404192, 48]\n",
      "[1.2423159925356773, 48]\n",
      "[1.2217451328918734, 46]\n",
      "[1.3955323305072116, 47]\n",
      "[1.2118590344129652, 46]\n",
      "[1.2578082358433116, 46]\n",
      "[0.41070226309705343, 45]\n",
      "[0.4351687690084976, 46]\n",
      "[0.4390055544484819, 48]\n",
      "[0.43917521048534447, 53]\n",
      "[0.4435888346286601, 44]\n",
      "[0.4310379515076859, 43]\n",
      "[0.4049875664056876, 43]\n",
      "[0.4041542578148206, 43]\n",
      "[0.4001651133464448, 42]\n",
      "[0.4001795061869929, 42]\n",
      "[0.3996749403671879, 42]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-be52431fb2e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     = tf_sess.run( \\\n\u001b[1;32m     30\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mtf_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_num_detections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         feed_dict={tf_input: imgRs[None, ...]})\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# index by 0 to remove batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "profile = pipeline.start(config)\n",
    "depth_sensor = profile.get_device().first_depth_sensor()\n",
    "depth_scale = depth_sensor.get_depth_scale()\n",
    "alp = 0.05\n",
    "res = 1 - alp**0.5\n",
    "val_size = 2\n",
    "values = [0,0]\n",
    "threshold = 3.\n",
    "#with serial.Serial('/dev/ttyACM0', 9600,timeout = 1)  as sr:\n",
    "#with serial.Serial('/dev/ttyACM0', 9600,timeout = 1)  as ser:\n",
    "while True:\n",
    "\n",
    "    # Caputure frame\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    RGB_frame = frames.get_color_frame()\n",
    "    img = np.asanyarray(RGB_frame.get_data())\n",
    "    imgConv = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    imgRs = cv2.resize(imgConv, (300, 300))\n",
    "    #imgRs = cv2.resize(imgConv, (640, 480))\n",
    "    if not depth_frame or not RGB_frame:\n",
    "        continue\n",
    "    # Do inference\n",
    "    scores, boxes, classes, num_detections \\\n",
    "    = tf_sess.run( \\\n",
    "        [tf_scores, tf_boxes, tf_classes, tf_num_detections], \\\n",
    "        feed_dict={tf_input: imgRs[None, ...]})\n",
    "\n",
    "    boxes = boxes[0] # index by 0 to remove batch dimension\n",
    "    scores = scores[0]\n",
    "    classes = classes[0]\n",
    "    num_detections = num_detections[0]\n",
    "    ls_d = []\n",
    "    ls_inf = []\n",
    "    ls_box = []\n",
    "    ls_ct = []\n",
    "    for i in range(int(num_detections)):\n",
    "        # Look up label string\n",
    "\n",
    "        class_id = int(classes[i])\n",
    "        label = labels[class_id] if class_id < num_labels else 'unlabeled'\n",
    "        if label == \"person\":\n",
    "        #if label == \"bottle\":\n",
    "            depth_image = np.asanyarray(depth_frame.get_data())\n",
    "            # Get score\n",
    "            score = scores[i]\n",
    "\n",
    "            # Draw bounding box\n",
    "            box = boxes[i] * np.array( \\\n",
    "                [FRAME_HEIGHT, FRAME_WIDTH, FRAME_HEIGHT, FRAME_WIDTH])\n",
    "            box = box.astype(np.int)\n",
    "            x1=box[1]\n",
    "            y1=box[0]\n",
    "            x2=box[3]\n",
    "            y2=box[2]\n",
    "            lx = (x2 - x1)\n",
    "            ud = int(lx*res/2)\n",
    "            u1 = x1 + ud\n",
    "            u2 = x2 - ud\n",
    "            ly = y2 - y1\n",
    "            vd = int(ly*res/2)\n",
    "            v1 = y1 + vd\n",
    "            v2 = y2 - vd\n",
    "            cv2.rectangle(img,(box[1], box[0]), (box[3], box[2]), (0, 255, 0), 3)\n",
    "            #cv2.rectangle(img,(box[1], box[0]+10), (box[3], box[2]-30), (255, 0, 0), 3)\n",
    "            cv2.rectangle(img, (u1, v1), (u2, v2), (255, 0, 0), 3)\n",
    "            cx = int(lx/2)\n",
    "            cy = int(ly/2)\n",
    "            #depth = depth_image[cx,cy].astype(float)\n",
    "            d_box = depth_image[v1:v2,u1:u2]\n",
    "            #send center val\n",
    "            center =(x1 + x2) / 2\n",
    "            #distance = depth * depth_scale\n",
    "            distance = np.mean(d_box) * depth_scale\n",
    "            # Put label near bounding box\n",
    "            inf = '%s: %f' % (label, score)\n",
    "            d = distance\n",
    "            ls_d.append(d)\n",
    "            ls_inf.append(inf)\n",
    "            ls_box.append(box[0:4])\n",
    "            ls_ct.append(center)\n",
    "            #print(inf)\n",
    "    if ls_d != []:   \n",
    "        min_i = ls_d.index(min(ls_d))\n",
    "        mbox = ls_box[min_i]\n",
    "                #min_center =ls_ct[min_i]\n",
    "        min_d=ls_d[min_i]\n",
    "        if min_d<=threshold:\n",
    "            #min_d = int(255*min_d/4.)\n",
    "            center = ls_ct[min_i]\n",
    "            center =int( 255 * (center /640.) )\n",
    "            values = [min_d,center]\n",
    "            print(values)\n",
    "            cv2.putText(img, ls_inf[min_i], (mbox[1], mbox[2]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(img, 'distance: %f' %(min_d), (mbox[1], mbox[2]-25), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 200, 100), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Show image\n",
    "    cv2.imshow(WINDOW_NAME, img)\n",
    "\n",
    "    # Check if user hits ESC key to exit\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27: # ESC \n",
    "        break\n",
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.stop()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
